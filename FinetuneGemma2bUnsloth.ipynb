{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d335a612",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-10T09:36:56.132190Z",
     "iopub.status.busy": "2025-03-10T09:36:56.131913Z",
     "iopub.status.idle": "2025-03-10T09:39:53.484263Z",
     "shell.execute_reply": "2025-03-10T09:39:53.483182Z"
    },
    "papermill": {
     "duration": 177.357489,
     "end_time": "2025-03-10T09:39:53.485905",
     "exception": false,
     "start_time": "2025-03-10T09:36:56.128416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth\r\n",
      "  Downloading unsloth-2025.3.9-py3-none-any.whl.metadata (59 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.3/59.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting unsloth_zoo>=2025.3.8 (from unsloth)\r\n",
      "  Downloading unsloth_zoo-2025.3.8-py3-none-any.whl.metadata (16 kB)\r\n",
      "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (2.5.1+cu121)\r\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth)\r\n",
      "  Downloading xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\r\n",
      "Collecting bitsandbytes (from unsloth)\r\n",
      "  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\r\n",
      "Collecting triton>=3.0.0 (from unsloth)\r\n",
      "  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth) (24.2)\r\n",
      "Collecting tyro (from unsloth)\r\n",
      "  Downloading tyro-0.9.16-py3-none-any.whl.metadata (9.4 kB)\r\n",
      "Collecting transformers!=4.47.0,>=4.46.1 (from unsloth)\r\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.3.1)\r\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.2.0)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.67.1)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth) (5.9.5)\r\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.45.1)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.26.4)\r\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.2.1)\r\n",
      "Collecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 (from unsloth)\r\n",
      "  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.14.0)\r\n",
      "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.20.3)\r\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.29.0)\r\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.1.9)\r\n",
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.31.0)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.20.1+cu121)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (0.4.5)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.17.0)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (19.0.1)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.2.3)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\r\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.12.0)\r\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.11.12)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->unsloth) (4.12.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->unsloth) (2.4.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.1.4)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.0)\r\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (13.9.4)\r\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.3.8->unsloth)\r\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\r\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2025.3.8->unsloth) (11.0.0)\r\n",
      "Collecting torch>=2.4.0 (from unsloth)\r\n",
      "  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.4.0->unsloth)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.4.0->unsloth)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.4.0->unsloth)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.0->unsloth)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->unsloth)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->unsloth)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->unsloth)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->unsloth)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->unsloth)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch>=2.4.0->unsloth)\r\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\r\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=2.4.0->unsloth)\r\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=2.4.0->unsloth)\r\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->unsloth)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers->unsloth) (8.5.0)\r\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Collecting torchvision (from unsloth)\r\n",
      "  Downloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\r\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (0.16)\r\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth)\r\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\r\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (4.4.1)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.4.6)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\r\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (5.0.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (25.1.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.1.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.2.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2025.1.31)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.19.1)\r\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->unsloth) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->unsloth) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->unsloth) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->unsloth) (2024.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->unsloth) (2024.2.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\r\n",
      "Downloading unsloth-2025.3.9-py3-none-any.whl (191 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.6/191.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading unsloth_zoo-2025.3.8-py3-none-any.whl (112 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl (43.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m974.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tyro-0.9.16-py3-none-any.whl (117 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\r\n",
      "Downloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\r\n",
      "Installing collected packages: triton, nvidia-cusparselt-cu12, shtab, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tyro, nvidia-cusolver-cu12, torch, cut_cross_entropy, transformers, trl, xformers, unsloth_zoo, torchvision, bitsandbytes, unsloth\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\r\n",
      "  Attempting uninstall: nvidia-nccl-cu12\r\n",
      "    Found existing installation: nvidia-nccl-cu12 2.23.4\r\n",
      "    Uninstalling nvidia-nccl-cu12-2.23.4:\r\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\r\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\r\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\r\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\r\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\r\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\r\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.5.1+cu121\r\n",
      "    Uninstalling torch-2.5.1+cu121:\r\n",
      "      Successfully uninstalled torch-2.5.1+cu121\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.47.0\r\n",
      "    Uninstalling transformers-4.47.0:\r\n",
      "      Successfully uninstalled transformers-4.47.0\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.20.1+cu121\r\n",
      "    Uninstalling torchvision-0.20.1+cu121:\r\n",
      "      Successfully uninstalled torchvision-0.20.1+cu121\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 25.2.0 which is incompatible.\r\n",
      "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.45.3 cut_cross_entropy-25.1.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 shtab-1.7.1 torch-2.6.0 torchvision-0.21.0 transformers-4.49.0 triton-3.2.0 trl-0.15.2 tyro-0.9.16 unsloth-2025.3.9 unsloth_zoo-2025.3.8 xformers-0.0.29.post3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a3a220",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T09:39:53.576730Z",
     "iopub.status.busy": "2025-03-10T09:39:53.576423Z",
     "iopub.status.idle": "2025-03-10T09:39:53.585518Z",
     "shell.execute_reply": "2025-03-10T09:39:53.584727Z"
    },
    "papermill": {
     "duration": 0.056013,
     "end_time": "2025-03-10T09:39:53.586798",
     "exception": false,
     "start_time": "2025-03-10T09:39:53.530785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing my_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_script.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# Import Unsloth and required trainer components\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Set up distributed training\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "# Initialize the distributed environment\n",
    "def setup_ddp():\n",
    "    # Check if we're running under torchrun/torch.distributed.launch\n",
    "    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
    "        rank = int(os.environ['RANK'])\n",
    "        world_size = int(os.environ['WORLD_SIZE'])\n",
    "        local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    else:\n",
    "        # For Kaggle with 2 GPUs, we can set these manually\n",
    "        world_size = torch.cuda.device_count()\n",
    "        rank = 0  # Master process\n",
    "        local_rank = 0\n",
    "    \n",
    "    if world_size > 1:\n",
    "        dist.init_process_group(backend='nccl', init_method='env://')\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        print(f\"Initialized process {rank}/{world_size} with local_rank {local_rank}\")\n",
    "    \n",
    "    return rank, world_size, local_rank\n",
    "\n",
    "# Configuration\n",
    "model_name = \"unsloth/gemma-2b-bnb-4bit\"  # Pre-quantized Gemma model\n",
    "output_dir = \"./gemma-9b-survey-finetuned\"\n",
    "max_seq_length = 1400  # Context window size\n",
    "load_in_4bit = True\n",
    "\n",
    "# Setup distributed training\n",
    "rank, world_size, local_rank = setup_ddp()\n",
    "is_main_process = (rank == 0)\n",
    "\n",
    "# Load your dataset (only on main process to avoid duplicating work)\n",
    "if is_main_process:\n",
    "    print(\"Loading data...\")\n",
    "    data_path = \"/kaggle/input/updateddatasethackathon/trainDataUpdatedFlag.csv\" \n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Loaded {len(df)} rows of data\")\n",
    "\n",
    "    # Check for NaN values in the Quality Flag column and drop them\n",
    "    nan_count = df[\"OE_Quality_Flag\"].isna().sum()\n",
    "    print(f\"Found {nan_count} NaN values in 'OE_Quality_Flag' column\")\n",
    "    if nan_count > 0:\n",
    "        original_count = len(df)\n",
    "        df = df.dropna(subset=[\"OE_Quality_Flag\"])\n",
    "        print(f\"Dropped {original_count - len(df)} rows with NaN values, {len(df)} rows remaining\")\n",
    "\n",
    "    # OPTIMIZATION 4: Sample a subset of the data for faster training\n",
    "    # Limit to 2000 examples while preserving class distribution\n",
    "    if len(df) > 2000:\n",
    "        # Get indices for each class\n",
    "        positive_indices = df[df[\"OE_Quality_Flag\"] == 1].index.tolist()\n",
    "        negative_indices = df[df[\"OE_Quality_Flag\"] == 0].index.tolist()\n",
    "        \n",
    "        # Calculate how many samples to take from each class (maintain proportion)\n",
    "        positive_ratio = len(positive_indices) / len(df)\n",
    "        positive_count = int(2000 * positive_ratio)\n",
    "        negative_count = 2000 - positive_count\n",
    "        \n",
    "        # Sample indices\n",
    "        sampled_positive = np.random.choice(positive_indices, min(positive_count, len(positive_indices)), replace=False)\n",
    "        sampled_negative = np.random.choice(negative_indices, min(negative_count, len(negative_indices)), replace=False)\n",
    "        \n",
    "        # Combine and filter the dataframe\n",
    "        sampled_indices = np.concatenate([sampled_positive, sampled_negative])\n",
    "        df = df.loc[sampled_indices].reset_index(drop=True)\n",
    "        print(f\"Sampled down to {len(df)} examples for faster training\")\n",
    "\n",
    "    # Function to convert a row to text (excluding specified columns)\n",
    "    def row_to_text(row, exclude_columns=[\"OE_Quality_Flag\", \"Unique ID\", \"Start Date\", \"End Date\"]):\n",
    "        details = []\n",
    "        for col in df.columns:\n",
    "            if col in exclude_columns:\n",
    "                continue\n",
    "            details.append(f\"{col}: {row[col]}\")\n",
    "        return \"\\n\".join(details)\n",
    "\n",
    "    # Prepare dataset texts and labels\n",
    "    print(\"Preparing dataset...\")\n",
    "    texts = df.apply(row_to_text, axis=1).tolist()\n",
    "    labels = df[\"OE_Quality_Flag\"].astype(int).tolist()\n",
    "\n",
    "    # Class distribution\n",
    "    positive_samples = sum(labels)\n",
    "    negative_samples = len(labels) - positive_samples\n",
    "    print(f\"Class distribution: Positive={positive_samples} ({positive_samples/len(labels):.2%}), Negative={negative_samples} ({negative_samples/len(labels):.2%})\")\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        texts, labels, test_size=0.2, random_state=seed, stratify=labels\n",
    "    )\n",
    "    print(f\"Data split: {len(train_texts)} training samples, {len(val_texts)} validation samples\")\n",
    "else:\n",
    "    train_texts, val_texts, train_labels, val_labels = None, None, None, None\n",
    "\n",
    "# Broadcast data to all processes if using multi-GPU\n",
    "if world_size > 1:\n",
    "    if rank == 0:\n",
    "        # Package data for broadcasting\n",
    "        data_package = {\n",
    "            'train_texts': train_texts,\n",
    "            'val_texts': val_texts,\n",
    "            'train_labels': train_labels,\n",
    "            'val_labels': val_labels\n",
    "        }\n",
    "    else:\n",
    "        data_package = None\n",
    "    \n",
    "    # Broadcast from rank 0 to all other processes\n",
    "    data_package = [data_package]\n",
    "    torch.distributed.broadcast_object_list(data_package, src=0)\n",
    "    data_package = data_package[0]\n",
    "    \n",
    "    # Unpack data\n",
    "    train_texts = data_package['train_texts']\n",
    "    val_texts = data_package['val_texts']\n",
    "    train_labels = data_package['train_labels']\n",
    "    val_labels = data_package['val_labels']\n",
    "\n",
    "# Load model and tokenizer using Unsloth (set device appropriately)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    device_map={\"\": local_rank} if world_size > 1 else \"auto\",\n",
    ")\n",
    "\n",
    "# OPTIMIZATION 1: Set up LoRA fine-tuning with reduced parameters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=8,  # Reduced from 16 to 8\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Reduced target modules\n",
    "    lora_alpha=16,  # Reduced from 32 to 16\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Uses less VRAM\n",
    "    random_state=seed,\n",
    ")\n",
    "\n",
    "# Apply the Gemma chat template\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"gemma\",  # Use Gemma's chat template\n",
    ")\n",
    "\n",
    "# Prepare the conversations for Gemma format\n",
    "dataset_list = []\n",
    "for text, label in zip(train_texts, train_labels):\n",
    "    conversation = [\n",
    "        {\"role\": \"user\", \"content\": f\"Analyze the following survey response and determine if it is a quality response.\\n\\n{text}\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"The quality flag for this survey is: {label}\"}\n",
    "    ]\n",
    "    dataset_list.append({\"conversations\": conversation})\n",
    "\n",
    "val_dataset_list = []\n",
    "for text, label in zip(train_texts, train_labels):\n",
    "    quality_text = \"not flagged (good quality)\" if label == 0 else \"flagged (poor quality)\"\n",
    "    conversation = [\n",
    "        {\"role\": \"user\", \"content\": f\"Analyze the following survey response and determine if it is a quality response.\\n\\n{text}\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"The quality flag for this survey is: {label} which means this response is {quality_text}.\"}\n",
    "    ]\n",
    "    val_dataset_list.append({\"conversations\": conversation})\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Dataset.from_list(dataset_list)\n",
    "val_dataset = Dataset.from_list(val_dataset_list)\n",
    "\n",
    "# Standardize format\n",
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "train_dataset = standardize_sharegpt(train_dataset)\n",
    "val_dataset = standardize_sharegpt(val_dataset)\n",
    "\n",
    "# Create formatting function\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            convo, tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "        for convo in convos\n",
    "    ]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply formatting\n",
    "train_dataset = train_dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    "    num_proc=4,  # OPTIMIZATION 4: Use multiprocessing for data prep\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    "    num_proc=4,  # OPTIMIZATION 4: Use multiprocessing for data prep\n",
    ")\n",
    "\n",
    "# Print sample to verify formatting (only on main process)\n",
    "if is_main_process:\n",
    "    print(\"\\nSample formatted instruction:\")\n",
    "    print(train_dataset[0][\"text\"][:500] + \"...\")\n",
    "\n",
    "# Create a custom callback to print losses and evaluation metrics\n",
    "from transformers import TrainerCallback, TrainerControl, TrainerState, TrainingArguments\n",
    "from typing import Dict\n",
    "\n",
    "class MetricsLoggingCallback(TrainerCallback):\n",
    "    def __init__(self, eval_dataset, tokenizer, model, val_texts, val_labels):\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.val_texts = val_texts\n",
    "        self.val_labels = val_labels\n",
    "        self.best_accuracy = 0.0\n",
    "        self.best_f1 = 0.0\n",
    "        \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        # Only log on the main process\n",
    "        if not dist.is_initialized() or dist.get_rank() == 0:\n",
    "            if logs is not None:\n",
    "                if \"loss\" in logs:\n",
    "                    print(f\"Step {state.global_step}: Training Loss = {logs['loss']:.4f}\")\n",
    "                if \"eval_loss\" in logs:\n",
    "                    print(f\"Step {state.global_step}: Validation Loss = {logs['eval_loss']:.4f}\")\n",
    "                \n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        # Only run evaluation on the main process\n",
    "        if not dist.is_initialized() or dist.get_rank() == 0:\n",
    "            # Run evaluation on a sample of validation data\n",
    "            print(f\"\\n===== Evaluation at step {state.global_step} =====\")\n",
    "            eval_subset = min(len(self.val_texts), 20)  # Start with a smaller subset\n",
    "            \n",
    "            predictions = []\n",
    "            for i in range(eval_subset):\n",
    "                text = self.val_texts[i]\n",
    "                pred = self.predict_sample(text)\n",
    "                predictions.append(pred)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            eval_labels = self.val_labels[:eval_subset]\n",
    "            accuracy = accuracy_score(eval_labels, predictions)\n",
    "            f1 = f1_score(eval_labels, predictions)\n",
    "            precision = precision_score(eval_labels, predictions, zero_division=0)\n",
    "            recall = recall_score(eval_labels, predictions, zero_division=0)\n",
    "            conf_matrix = confusion_matrix(eval_labels, predictions)\n",
    "            \n",
    "            # Update best scores\n",
    "            if accuracy > self.best_accuracy:\n",
    "                self.best_accuracy = accuracy\n",
    "            if f1 > self.best_f1:\n",
    "                self.best_f1 = f1\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"Evaluating on {eval_subset} samples:\")\n",
    "            print(f\"Accuracy: {accuracy:.4f} (Best: {self.best_accuracy:.4f})\")\n",
    "            print(f\"F1 Score: {f1:.4f} (Best: {self.best_f1:.4f})\")\n",
    "            print(f\"Precision: {precision:.4f}\")\n",
    "            print(f\"Recall: {recall:.4f}\")\n",
    "            print(f\"Confusion Matrix:\")\n",
    "            print(conf_matrix)\n",
    "            \n",
    "            # Store metrics in logs for potential saving\n",
    "            if metrics is not None:\n",
    "                metrics[\"eval_accuracy\"] = accuracy\n",
    "                metrics[\"eval_f1\"] = f1\n",
    "                metrics[\"eval_precision\"] = precision\n",
    "                metrics[\"eval_recall\"] = recall\n",
    "            \n",
    "            print(\"=\" * 50)\n",
    "        \n",
    "    def predict_sample(self, text):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"Analyze the following survey response and determine if it is a quality response.\\n\\n{text}\"}\n",
    "        ]\n",
    "        inputs = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        # Generate prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids=inputs,\n",
    "                max_new_tokens=20,\n",
    "                temperature=0.1,\n",
    "                use_cache=True\n",
    "            )\n",
    "        \n",
    "        # Decode and extract prediction\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Try different ways to extract the prediction (0 or 1)\n",
    "        try:\n",
    "            if \"quality flag\" in generated_text.lower() and \":\" in generated_text:\n",
    "                # Extract the number after \"quality flag:\"\n",
    "                prediction_text = generated_text.lower().split(\"quality flag\")[-1].split(\":\")[1].strip()\n",
    "                prediction = int(prediction_text[0])  # Take first digit\n",
    "            elif \"0\" in generated_text:\n",
    "                prediction = 0\n",
    "            elif \"1\" in generated_text:\n",
    "                prediction = 1\n",
    "            else:\n",
    "                prediction = 0  # Default\n",
    "        except:\n",
    "            prediction = 0  # Default if parsing fails\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "# OPTIMIZATION 6: Add early stopping callback\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.01\n",
    ")\n",
    "\n",
    "# Define helper function to check bfloat16 support\n",
    "def is_bfloat16_supported():\n",
    "    return hasattr(torch.cuda, \"is_bf16_supported\") and torch.cuda.is_bf16_supported()\n",
    "\n",
    "# Set up training arguments with distributed training options\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,  # Explicitly disable fp16\n",
    "    bf16=is_bfloat16_supported(),  # Use bf16 if supported\n",
    "    logging_steps=10,  # Log less frequently (changed from 1)\n",
    "    logging_first_step=True,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"steps\",  # Using new parameter name instead of deprecated evaluation_strategy\n",
    "    eval_steps=20,  # Evaluate less frequently (changed from 5)\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\",\n",
    "    seed=seed,\n",
    "    # Distributed training specific args\n",
    "    local_rank=local_rank,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    ddp_bucket_cap_mb=25,\n",
    "    dataloader_pin_memory=False,\n",
    "    # OPTIMIZATION 4: Add data loading optimizations\n",
    "    dataloader_num_workers=4,  # Use multiple workers for data loading\n",
    ")\n",
    "\n",
    "# Initialize our custom callback - only used on main process\n",
    "if is_main_process:\n",
    "    custom_callback = [\n",
    "        MetricsLoggingCallback(\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            val_texts=val_texts,\n",
    "            val_labels=val_labels\n",
    "        ),\n",
    "        early_stopping_callback  # OPTIMIZATION 6: Add early stopping\n",
    "    ]\n",
    "else:\n",
    "    custom_callback = [early_stopping_callback]  # OPTIMIZATION 6: Add early stopping on all processes\n",
    "\n",
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "    dataset_num_proc=4,  # OPTIMIZATION 4: Use multiprocessing\n",
    "    packing=False,\n",
    "    args=training_args,\n",
    "    callbacks=custom_callback,\n",
    ")\n",
    "\n",
    "# Set up to train on assistant responses only\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    # Adjust these based on actual template inspection\n",
    "    instruction_part=\"<start_of_turn>user\",\n",
    "    response_part=\"<start_of_turn>model\",\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "if is_main_process:\n",
    "    print(\"\\nStarting training...\")\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# Print final training metrics (only on main process)\n",
    "if is_main_process:\n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(f\"Final training loss: {trainer_stats.training_loss}\")\n",
    "\n",
    "    # Convert model to inference mode\n",
    "    FastLanguageModel.for_inference(model)\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    print(\"\\nSaving model...\")\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model saved to {output_dir}\")\n",
    "\n",
    "    # Comprehensive evaluation function to evaluate on the full validation set\n",
    "    def evaluate_model_full():\n",
    "        print(\"\\n===== FINAL MODEL EVALUATION =====\")\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Function to predict\n",
    "        def predict(text):\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": f\"Analyze the following survey response and determine if it is a quality response.\\n\\n{text}\"}\n",
    "            ]\n",
    "            inputs = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(model.device)\n",
    "            \n",
    "            # Generate prediction\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    input_ids=inputs,\n",
    "                    max_new_tokens=20,\n",
    "                    temperature=0.1,\n",
    "                    use_cache=True\n",
    "                )\n",
    "            \n",
    "            # Decode and extract prediction\n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # More robust prediction extraction\n",
    "            try:\n",
    "                if \"quality flag\" in generated_text.lower() and \":\" in generated_text:\n",
    "                    prediction_text = generated_text.lower().split(\"quality flag\")[-1].split(\":\")[1].strip()\n",
    "                    prediction = int(prediction_text[0])\n",
    "                elif \"0\" in generated_text:\n",
    "                    prediction = 0\n",
    "                elif \"1\" in generated_text:\n",
    "                    prediction = 1\n",
    "                else:\n",
    "                    prediction = 0\n",
    "            except:\n",
    "                prediction = 0\n",
    "                \n",
    "            return prediction, generated_text\n",
    "        \n",
    "        # Evaluate on full validation set\n",
    "        print(f\"Evaluating on all {len(val_texts)} validation samples...\")\n",
    "        val_predictions = []\n",
    "        all_generated_texts = []\n",
    "        \n",
    "        for idx, text in enumerate(val_texts):\n",
    "            pred, gen_text = predict(text)\n",
    "            val_predictions.append(pred)\n",
    "            all_generated_texts.append(gen_text)\n",
    "            \n",
    "            # Print progress every 5 samples\n",
    "            if (idx + 1) % 5 == 0 or idx == len(val_texts) - 1:\n",
    "                print(f\"Processed {idx + 1}/{len(val_texts)} samples...\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(val_labels, val_predictions)\n",
    "        f1 = f1_score(val_labels, val_predictions)\n",
    "        precision = precision_score(val_labels, val_predictions, zero_division=0)\n",
    "        recall = recall_score(val_labels, val_predictions, zero_division=0)\n",
    "        conf_matrix = confusion_matrix(val_labels, val_predictions)\n",
    "        \n",
    "        # Print detailed metrics\n",
    "        print(\"\\n===== FINAL EVALUATION RESULTS =====\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"Confusion Matrix:\")\n",
    "        print(conf_matrix)\n",
    "        \n",
    "        # Calculate class-specific metrics\n",
    "        tn, fp, fn, tp = conf_matrix.ravel()\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        \n",
    "        print(f\"\\nTrue Positives: {tp}\")\n",
    "        print(f\"True Negatives: {tn}\")\n",
    "        print(f\"False Positives: {fp}\")\n",
    "        print(f\"False Negatives: {fn}\")\n",
    "        print(f\"Specificity (True Negative Rate): {specificity:.4f}\")\n",
    "        \n",
    "        # Print some example predictions\n",
    "        print(\"\\n===== EXAMPLE PREDICTIONS =====\")\n",
    "        for i in range(min(5, len(val_texts))):\n",
    "            print(f\"\\nSample {i+1}:\")\n",
    "            print(f\"True label: {val_labels[i]}\")\n",
    "            print(f\"Predicted: {val_predictions[i]}\")\n",
    "            print(f\"Generated text: {all_generated_texts[i][:100]}...\")\n",
    "        \n",
    "        # Save predictions to CSV for further analysis\n",
    "        results_df = pd.DataFrame({\n",
    "            \"true_label\": val_labels,\n",
    "            \"predicted_label\": val_predictions,\n",
    "            \"generated_text\": all_generated_texts\n",
    "        })\n",
    "        \n",
    "        results_path = os.path.join(output_dir, \"validation_results.csv\")\n",
    "        results_df.to_csv(results_path, index=False)\n",
    "        print(f\"\\nDetailed validation results saved to {results_path}\")\n",
    "        \n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1\": f1,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"specificity\": specificity,\n",
    "            \"confusion_matrix\": conf_matrix\n",
    "        }\n",
    "\n",
    "    # Run the comprehensive evaluation\n",
    "    print(\"\\nRunning final model evaluation...\")\n",
    "    final_metrics = evaluate_model_full()\n",
    "\n",
    "    print(\"\\nFine-tuning and evaluation complete!\")\n",
    "\n",
    "# Cleanup distributed process group if initialized\n",
    "if world_size > 1:\n",
    "    dist.destroy_process_group()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d599ff5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T09:39:53.676287Z",
     "iopub.status.busy": "2025-03-10T09:39:53.675999Z",
     "iopub.status.idle": "2025-03-10T09:39:53.795675Z",
     "shell.execute_reply": "2025-03-10T09:39:53.794442Z"
    },
    "papermill": {
     "duration": 0.166834,
     "end_time": "2025-03-10T09:39:53.797531",
     "exception": false,
     "start_time": "2025-03-10T09:39:53.630697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a293fb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T09:39:53.887794Z",
     "iopub.status.busy": "2025-03-10T09:39:53.887470Z",
     "iopub.status.idle": "2025-03-10T10:40:31.251789Z",
     "shell.execute_reply": "2025-03-10T10:40:31.250718Z"
    },
    "papermill": {
     "duration": 3637.411403,
     "end_time": "2025-03-10T10:40:31.253512",
     "exception": false,
     "start_time": "2025-03-10T09:39:53.842109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\r\n",
      "and will be removed in future. Use torchrun.\r\n",
      "Note that --use-env is set by default in torchrun.\r\n",
      "If your script expects `--local-rank` argument to be set, please\r\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \r\n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \r\n",
      "further instructions\r\n",
      "\r\n",
      "  main()\r\n",
      "W0310 09:39:55.905000 62 torch/distributed/run.py:792] \r\n",
      "W0310 09:39:55.905000 62 torch/distributed/run.py:792] *****************************************\r\n",
      "W0310 09:39:55.905000 62 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n",
      "W0310 09:39:55.905000 62 torch/distributed/run.py:792] *****************************************\r\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\r\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\r\n",
      "2025-03-10 09:40:07.832564: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-03-10 09:40:07.832557: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-03-10 09:40:08.042396: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2025-03-10 09:40:08.042395: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2025-03-10 09:40:08.102189: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-03-10 09:40:08.102198: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\r\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\r\n",
      "Initialized process 0/2 with local_rank 0\r\n",
      "Loading data...\r\n",
      "Loaded 2300 rows of data\r\n",
      "Found 1 NaN values in 'OE_Quality_Flag' column\r\n",
      "Dropped 1 rows with NaN values, 2299 rows remaining\r\n",
      "Sampled down to 2000 examples for faster training\r\n",
      "Preparing dataset...\r\n",
      "Initialized process 1/2 with local_rank 1\r\n",
      "Class distribution: Positive=156 (7.80%), Negative=1844 (92.20%)\r\n",
      "Data split: 1600 training samples, 400 validation samples\r\n",
      "==((====))==  Unsloth 2025.3.9: Fast Gemma patching. Transformers: 4.49.0.\r\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\r\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\r\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\r\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\r\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\r\n",
      "==((====))==  Unsloth 2025.3.9: Fast Gemma patching. Transformers: 4.49.0.\r\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\r\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\r\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\r\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\r\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\r\n",
      "model.safetensors: 100%|████████████████████| 2.07G/2.07G [00:13<00:00, 154MB/s]\r\n",
      "generation_config.json: 100%|███████████████████| 154/154 [00:00<00:00, 845kB/s]\r\n",
      "tokenizer_config.json: 100%|███████████████| 40.0k/40.0k [00:00<00:00, 13.7MB/s]\r\n",
      "tokenizer.model: 100%|█████████████████████| 4.24M/4.24M [00:00<00:00, 26.8MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████████| 636/636 [00:00<00:00, 3.66MB/s]\r\n",
      "tokenizer.json: 100%|███████████████████████| 17.5M/17.5M [00:00<00:00, 128MB/s]\r\n",
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\r\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\r\n",
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\r\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\r\n",
      "Unsloth 2025.3.9 patched 18 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\r\n",
      "Unsloth 2025.3.9 patched 18 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\r\n",
      "Unsloth: Will map <end_of_turn> to EOS = <eos>.\r\n",
      "Unsloth: Will map <end_of_turn> to EOS = <eos>.\r\n",
      "Standardizing format: 100%|███████| 1600/1600 [00:00<00:00, 21908.73 examples/s]\r\n",
      "Standardizing format: 100%|███████| 1600/1600 [00:00<00:00, 28357.26 examples/s]\r\n",
      "Standardizing format: 100%|███████| 1600/1600 [00:00<00:00, 28431.26 examples/s]\r\n",
      "Standardizing format: 100%|███████| 1600/1600 [00:00<00:00, 28835.87 examples/s]\r\n",
      "Map (num_proc=4): 100%|████████████| 1600/1600 [00:01<00:00, 1395.87 examples/s]\r\n",
      "Map (num_proc=4): 100%|████████████| 1600/1600 [00:01<00:00, 1461.00 examples/s]\r\n",
      "Map (num_proc=4): 100%|████████████| 1600/1600 [00:00<00:00, 1659.79 examples/s]\r\n",
      "Map (num_proc=4): 100%|████████████| 1600/1600 [00:01<00:00, 1559.20 examples/s]\r\n",
      "Unsloth: We found double BOS tokens - we shall remove one automatically.\r\n",
      "\r\n",
      "Sample formatted instruction:\r\n",
      "<bos><start_of_turn>user\r\n",
      "Analyze the following survey response and determine if it is a quality response.\r\n",
      "\r\n",
      "Q1. What is your current age? \r\n",
      "(Age): 25.0\r\n",
      "Q2. What is your gender? \r\n",
      "(Gender): Male\r\n",
      "Q3. Which of the following best describes the area or community in which you live? \r\n",
      "(Urban/Rural): Big city\r\n",
      "Q4.  Please indicate the answer that includes your entire household income in (previous year) before taxes. \r\n",
      "(Income): $50,000 to $59,999\r\n",
      "Q6 Which of the following types of alcoholic beverages have you...\r\n",
      "Unsloth: We found double BOS tokens - we shall remove one automatically.\r\n",
      "Tokenizing to [\"text\"] (num_proc=4): 100%|█| 1600/1600 [00:11<00:00, 133.54 exam\r\n",
      "Unsloth: We found double BOS tokens - we shall remove one automatically.\r\n",
      "Tokenizing to [\"text\"] (num_proc=4): 100%|█| 1600/1600 [00:12<00:00, 129.73 exam\r\n",
      "Unsloth: We found double BOS tokens - we shall remove one automatically.\r\n",
      "Tokenizing to [\"text\"] (num_proc=4): 100%|█| 1600/1600 [00:12<00:00, 130.31 exam\r\n",
      "Tokenizing to [\"text\"] (num_proc=4): 100%|█| 1600/1600 [00:12<00:00, 129.05 exam\r\n",
      "Map (num_proc=4): 100%|█████████████| 1600/1600 [00:02<00:00, 555.53 examples/s]\r\n",
      "Map (num_proc=4): 100%|█████████████| 1600/1600 [00:02<00:00, 568.28 examples/s]\r\n",
      "Map (num_proc=4): 100%|█████████████| 1600/1600 [00:02<00:00, 560.77 examples/s]\r\n",
      "\r\n",
      "Starting training...\r\n",
      "Map (num_proc=4): 100%|█████████████| 1600/1600 [00:03<00:00, 522.65 examples/s]\r\n",
      "Unsloth is running with multi GPUs - the effective batch size is multiplied by 2\r\n",
      "Unsloth is running with multi GPUs - the effective batch size is multiplied by 2\r\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\r\n",
      "   \\\\   /|    Num examples = 1,600 | Num Epochs = 3 | Total steps = 36\r\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient accumulation steps = 4\r\n",
      "\\        /    Data Parallel GPUs = 2 | Total batch size (16 x 4 x 2) = 128\r\n",
      " \"-____-\"     Trainable parameters = 1,843,200/1,517,111,296 (0.12% trained)\r\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\r\n",
      "   \\\\   /|    Num examples = 1,600 | Num Epochs = 3 | Total steps = 36\r\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient accumulation steps = 4\r\n",
      "\\        /    Data Parallel GPUs = 2 | Total batch size (16 x 4 x 2) = 128\r\n",
      " \"-____-\"     Trainable parameters = 1,843,200/1,517,111,296 (0.12% trained)\r\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]Unsloth: Will smartly offload gradients to save VRAM!\r\n",
      "Unsloth: Will smartly offload gradients to save VRAM!\r\n",
      "  3%|█▏                                          | 1/36 [01:20<47:03, 80.67s/it]Step 1: Training Loss = 2.7442\r\n",
      "{'loss': 2.7442, 'grad_norm': 2.681239366531372, 'learning_rate': 0.0001, 'epoch': 0.08}\r\n",
      " 28%|███████████▉                               | 10/36 [14:02<36:58, 85.34s/it]Step 10: Training Loss = 1.8338\r\n",
      "{'loss': 1.8338, 'grad_norm': 2.3146209716796875, 'learning_rate': 0.00017390089172206592, 'epoch': 0.8}\r\n",
      " 56%|███████████████████████▉                   | 20/36 [27:12<21:39, 81.20s/it]Step 20: Training Loss = 0.2891\r\n",
      "{'loss': 0.2891, 'grad_norm': 0.5873333811759949, 'learning_rate': 9.077316405366981e-05, 'epoch': 1.56}\r\n",
      " 56%|███████████████████████▉                   | 20/36 [27:12<21:39, 81.20s/it]Unsloth: Not an error, but GemmaForCausalLM does not accept `num_items_in_batch`.\r\n",
      "Using gradient accumulation will be very slightly less accurate.\r\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\r\n",
      "Unsloth: Not an error, but GemmaForCausalLM does not accept `num_items_in_batch`.\r\n",
      "Using gradient accumulation will be very slightly less accurate.\r\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\r\n",
      "\r\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]\u001b[A\r\n",
      "  2%|▊                                          | 2/100 [00:03<02:48,  1.72s/it]\u001b[A\r\n",
      "  3%|█▎                                         | 3/100 [00:07<04:36,  2.85s/it]\u001b[A\r\n",
      "  4%|█▋                                         | 4/100 [00:11<04:56,  3.09s/it]\u001b[A\r\n",
      "  5%|██▏                                        | 5/100 [00:14<05:08,  3.24s/it]\u001b[A\r\n",
      "  6%|██▌                                        | 6/100 [00:18<05:08,  3.28s/it]\u001b[A\r\n",
      "  7%|███                                        | 7/100 [00:21<05:12,  3.36s/it]\u001b[A\r\n",
      "  8%|███▍                                       | 8/100 [00:25<05:08,  3.35s/it]\u001b[A\r\n",
      "  9%|███▊                                       | 9/100 [00:28<05:14,  3.46s/it]\u001b[A\r\n",
      " 10%|████▏                                     | 10/100 [00:32<05:16,  3.52s/it]\u001b[A\r\n",
      " 11%|████▌                                     | 11/100 [00:35<05:08,  3.47s/it]\u001b[A\r\n",
      " 12%|█████                                     | 12/100 [00:39<05:23,  3.68s/it]\u001b[A\r\n",
      " 13%|█████▍                                    | 13/100 [00:43<05:08,  3.55s/it]\u001b[A\r\n",
      " 14%|█████▉                                    | 14/100 [00:46<05:02,  3.52s/it]\u001b[A\r\n",
      " 15%|██████▎                                   | 15/100 [00:50<04:58,  3.51s/it]\u001b[A\r\n",
      " 16%|██████▋                                   | 16/100 [00:53<04:55,  3.51s/it]\u001b[A\r\n",
      " 17%|███████▏                                  | 17/100 [00:57<04:51,  3.51s/it]\u001b[A\r\n",
      " 18%|███████▌                                  | 18/100 [01:00<04:45,  3.48s/it]\u001b[A\r\n",
      " 19%|███████▉                                  | 19/100 [01:04<04:40,  3.46s/it]\u001b[A\r\n",
      " 20%|████████▍                                 | 20/100 [01:07<04:37,  3.47s/it]\u001b[A\r\n",
      " 21%|████████▊                                 | 21/100 [01:10<04:33,  3.47s/it]\u001b[A\r\n",
      " 22%|█████████▏                                | 22/100 [01:14<04:34,  3.51s/it]\u001b[A\r\n",
      " 23%|█████████▋                                | 23/100 [01:18<04:39,  3.62s/it]\u001b[A\r\n",
      " 24%|██████████                                | 24/100 [01:21<04:29,  3.54s/it]\u001b[A\r\n",
      " 25%|██████████▌                               | 25/100 [01:24<04:17,  3.43s/it]\u001b[A\r\n",
      " 26%|██████████▉                               | 26/100 [01:28<04:18,  3.49s/it]\u001b[A\r\n",
      " 27%|███████████▎                              | 27/100 [01:32<04:15,  3.50s/it]\u001b[A\r\n",
      " 28%|███████████▊                              | 28/100 [01:35<04:08,  3.45s/it]\u001b[A\r\n",
      " 29%|████████████▏                             | 29/100 [01:38<04:05,  3.46s/it]\u001b[A\r\n",
      " 30%|████████████▌                             | 30/100 [01:42<04:10,  3.57s/it]\u001b[A\r\n",
      " 31%|█████████████                             | 31/100 [01:45<03:58,  3.46s/it]\u001b[A\r\n",
      " 32%|█████████████▍                            | 32/100 [01:49<03:53,  3.44s/it]\u001b[A\r\n",
      " 33%|█████████████▊                            | 33/100 [01:52<03:52,  3.47s/it]\u001b[A\r\n",
      " 34%|██████████████▎                           | 34/100 [01:56<03:47,  3.44s/it]\u001b[A\r\n",
      " 35%|██████████████▋                           | 35/100 [01:59<03:41,  3.40s/it]\u001b[A\r\n",
      " 36%|███████████████                           | 36/100 [02:02<03:36,  3.38s/it]\u001b[A\r\n",
      " 37%|███████████████▌                          | 37/100 [02:06<03:33,  3.39s/it]\u001b[A\r\n",
      " 38%|███████████████▉                          | 38/100 [02:09<03:32,  3.43s/it]\u001b[A\r\n",
      " 39%|████████████████▍                         | 39/100 [02:13<03:30,  3.45s/it]\u001b[A\r\n",
      " 40%|████████████████▊                         | 40/100 [02:16<03:26,  3.45s/it]\u001b[A\r\n",
      " 41%|█████████████████▏                        | 41/100 [02:20<03:23,  3.45s/it]\u001b[A\r\n",
      " 42%|█████████████████▋                        | 42/100 [02:23<03:19,  3.44s/it]\u001b[A\r\n",
      " 43%|██████████████████                        | 43/100 [02:27<03:14,  3.42s/it]\u001b[A\r\n",
      " 44%|██████████████████▍                       | 44/100 [02:30<03:12,  3.45s/it]\u001b[A\r\n",
      " 45%|██████████████████▉                       | 45/100 [02:33<03:08,  3.43s/it]\u001b[A\r\n",
      " 46%|███████████████████▎                      | 46/100 [02:37<03:11,  3.55s/it]\u001b[A\r\n",
      " 47%|███████████████████▋                      | 47/100 [02:40<03:01,  3.43s/it]\u001b[A\r\n",
      " 48%|████████████████████▏                     | 48/100 [02:44<03:00,  3.47s/it]\u001b[A\r\n",
      " 49%|████████████████████▌                     | 49/100 [02:47<02:55,  3.43s/it]\u001b[A\r\n",
      " 50%|█████████████████████                     | 50/100 [02:51<02:56,  3.53s/it]\u001b[A\r\n",
      " 51%|█████████████████████▍                    | 51/100 [02:54<02:47,  3.42s/it]\u001b[A\r\n",
      " 52%|█████████████████████▊                    | 52/100 [02:58<02:41,  3.37s/it]\u001b[A\r\n",
      " 53%|██████████████████████▎                   | 53/100 [03:01<02:43,  3.47s/it]\u001b[A\r\n",
      " 54%|██████████████████████▋                   | 54/100 [03:05<02:38,  3.44s/it]\u001b[A\r\n",
      " 55%|███████████████████████                   | 55/100 [03:08<02:35,  3.45s/it]\u001b[A\r\n",
      " 56%|███████████████████████▌                  | 56/100 [03:11<02:30,  3.43s/it]\u001b[A\r\n",
      " 57%|███████████████████████▉                  | 57/100 [03:15<02:29,  3.47s/it]\u001b[A\r\n",
      " 58%|████████████████████████▎                 | 58/100 [03:19<02:26,  3.48s/it]\u001b[A\r\n",
      " 59%|████████████████████████▊                 | 59/100 [03:22<02:20,  3.43s/it]\u001b[A\r\n",
      " 60%|█████████████████████████▏                | 60/100 [03:25<02:18,  3.47s/it]\u001b[A\r\n",
      " 61%|█████████████████████████▌                | 61/100 [03:29<02:15,  3.46s/it]\u001b[A\r\n",
      " 62%|██████████████████████████                | 62/100 [03:32<02:12,  3.49s/it]\u001b[A\r\n",
      " 63%|██████████████████████████▍               | 63/100 [03:36<02:08,  3.47s/it]\u001b[A\r\n",
      " 64%|██████████████████████████▉               | 64/100 [03:39<02:04,  3.45s/it]\u001b[A\r\n",
      " 65%|███████████████████████████▎              | 65/100 [03:43<02:03,  3.53s/it]\u001b[A\r\n",
      " 66%|███████████████████████████▋              | 66/100 [03:46<01:59,  3.51s/it]\u001b[A\r\n",
      " 67%|████████████████████████████▏             | 67/100 [03:50<01:55,  3.51s/it]\u001b[A\r\n",
      " 68%|████████████████████████████▌             | 68/100 [03:53<01:51,  3.49s/it]\u001b[A\r\n",
      " 69%|████████████████████████████▉             | 69/100 [03:57<01:52,  3.63s/it]\u001b[A\r\n",
      " 70%|█████████████████████████████▍            | 70/100 [04:01<01:48,  3.63s/it]\u001b[A\r\n",
      " 71%|█████████████████████████████▊            | 71/100 [04:05<01:46,  3.66s/it]\u001b[A\r\n",
      " 72%|██████████████████████████████▏           | 72/100 [04:08<01:40,  3.58s/it]\u001b[A\r\n",
      " 73%|██████████████████████████████▋           | 73/100 [04:12<01:35,  3.55s/it]\u001b[A\r\n",
      " 74%|███████████████████████████████           | 74/100 [04:15<01:33,  3.61s/it]\u001b[A\r\n",
      " 75%|███████████████████████████████▌          | 75/100 [04:19<01:27,  3.51s/it]\u001b[A\r\n",
      " 76%|███████████████████████████████▉          | 76/100 [04:22<01:24,  3.51s/it]\u001b[A\r\n",
      " 77%|████████████████████████████████▎         | 77/100 [04:26<01:21,  3.53s/it]\u001b[A\r\n",
      " 78%|████████████████████████████████▊         | 78/100 [04:29<01:17,  3.51s/it]\u001b[A\r\n",
      " 79%|█████████████████████████████████▏        | 79/100 [04:33<01:13,  3.51s/it]\u001b[A\r\n",
      " 80%|█████████████████████████████████▌        | 80/100 [04:36<01:10,  3.52s/it]\u001b[A\r\n",
      " 81%|██████████████████████████████████        | 81/100 [04:40<01:06,  3.48s/it]\u001b[A\r\n",
      " 82%|██████████████████████████████████▍       | 82/100 [04:43<01:02,  3.46s/it]\u001b[A\r\n",
      " 83%|██████████████████████████████████▊       | 83/100 [04:46<00:59,  3.47s/it]\u001b[A\r\n",
      " 84%|███████████████████████████████████▎      | 84/100 [04:50<00:54,  3.42s/it]\u001b[A\r\n",
      " 85%|███████████████████████████████████▋      | 85/100 [04:53<00:51,  3.45s/it]\u001b[A\r\n",
      " 86%|████████████████████████████████████      | 86/100 [04:57<00:48,  3.48s/it]\u001b[A\r\n",
      " 87%|████████████████████████████████████▌     | 87/100 [05:00<00:44,  3.43s/it]\u001b[A\r\n",
      " 88%|████████████████████████████████████▉     | 88/100 [05:04<00:41,  3.43s/it]\u001b[A\r\n",
      " 89%|█████████████████████████████████████▍    | 89/100 [05:07<00:37,  3.43s/it]\u001b[A\r\n",
      " 90%|█████████████████████████████████████▊    | 90/100 [05:10<00:34,  3.41s/it]\u001b[A\r\n",
      " 91%|██████████████████████████████████████▏   | 91/100 [05:14<00:32,  3.57s/it]\u001b[A\r\n",
      " 92%|██████████████████████████████████████▋   | 92/100 [05:18<00:27,  3.46s/it]\u001b[A\r\n",
      " 93%|███████████████████████████████████████   | 93/100 [05:21<00:24,  3.49s/it]\u001b[A\r\n",
      " 94%|███████████████████████████████████████▍  | 94/100 [05:25<00:20,  3.48s/it]\u001b[A\r\n",
      " 95%|███████████████████████████████████████▉  | 95/100 [05:28<00:17,  3.45s/it]\u001b[A\r\n",
      " 96%|████████████████████████████████████████▎ | 96/100 [05:31<00:13,  3.45s/it]\u001b[A\r\n",
      " 97%|████████████████████████████████████████▋ | 97/100 [05:35<00:10,  3.47s/it]\u001b[A\r\n",
      " 98%|█████████████████████████████████████████▏| 98/100 [05:38<00:06,  3.47s/it]\u001b[A\r\n",
      " 99%|█████████████████████████████████████████▌| 99/100 [05:42<00:03,  3.49s/it]\u001b[A\r\n",
      "100%|█████████████████████████████████████████| 100/100 [05:46<00:00,  3.56s/it]\u001b[AStep 20: Validation Loss = 2.8556\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 2.8555970191955566, 'eval_runtime': 351.1728, 'eval_samples_per_second': 4.556, 'eval_steps_per_second': 0.285, 'epoch': 1.56}\r\n",
      " 56%|███████████████████████▉                   | 20/36 [33:03<21:39, 81.20s/it]\r\n",
      "100%|█████████████████████████████████████████| 100/100 [05:46<00:00,  3.56s/it]\u001b[A\r\n",
      "===== Evaluation at step 20 =====\r\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\n",
      "Evaluating on 20 samples:\r\n",
      "Accuracy: 0.9500 (Best: 0.9500)\r\n",
      "F1 Score: 0.0000 (Best: 0.0000)\r\n",
      "Precision: 0.0000\r\n",
      "Recall: 0.0000\r\n",
      "Confusion Matrix:\r\n",
      "[[19  0]\r\n",
      " [ 1  0]]\r\n",
      "==================================================\r\n",
      "\r\n",
      " 83%|███████████████████████████████████▊       | 30/36 [45:56<08:06, 81.02s/it]Step 30: Training Loss = 0.0255\r\n",
      "{'loss': 0.0255, 'grad_norm': 0.22670336067676544, 'learning_rate': 1.4978286427038601e-05, 'epoch': 2.32}\r\n",
      "100%|███████████████████████████████████████████| 36/36 [53:50<00:00, 79.69s/it]Could not locate the best model at ./gemma-9b-survey-finetuned/checkpoint-20/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\r\n",
      "Could not locate the best model at ./gemma-9b-survey-finetuned/checkpoint-20/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\r\n",
      "{'train_runtime': 3231.4053, 'train_samples_per_second': 1.485, 'train_steps_per_second': 0.011, 'train_loss': 0.6265437391897043, 'epoch': 2.8}\r\n",
      "100%|███████████████████████████████████████████| 36/36 [53:51<00:00, 89.76s/it]\r\n",
      "\r\n",
      "Training completed!\r\n",
      "Final training loss: 0.6265437391897043\r\n",
      "\r\n",
      "Saving model...\r\n",
      "Model saved to ./gemma-9b-survey-finetuned\r\n",
      "\r\n",
      "Running final model evaluation...\r\n",
      "\r\n",
      "===== FINAL MODEL EVALUATION =====\r\n",
      "Evaluating on all 400 validation samples...\r\n",
      "Processed 5/400 samples...\r\n",
      "Processed 10/400 samples...\r\n",
      "Processed 15/400 samples...\r\n",
      "Processed 20/400 samples...\r\n",
      "Processed 25/400 samples...\r\n",
      "Processed 30/400 samples...\r\n",
      "Processed 35/400 samples...\r\n",
      "Processed 40/400 samples...\r\n",
      "Processed 45/400 samples...\r\n",
      "Processed 50/400 samples...\r\n",
      "Processed 55/400 samples...\r\n",
      "Processed 60/400 samples...\r\n",
      "Processed 65/400 samples...\r\n",
      "Processed 70/400 samples...\r\n",
      "Processed 75/400 samples...\r\n",
      "Processed 80/400 samples...\r\n",
      "Processed 85/400 samples...\r\n",
      "Processed 90/400 samples...\r\n",
      "Processed 95/400 samples...\r\n",
      "Processed 100/400 samples...\r\n",
      "Processed 105/400 samples...\r\n",
      "Processed 110/400 samples...\r\n",
      "Processed 115/400 samples...\r\n",
      "Processed 120/400 samples...\r\n",
      "Processed 125/400 samples...\r\n",
      "Processed 130/400 samples...\r\n",
      "Processed 135/400 samples...\r\n",
      "Processed 140/400 samples...\r\n",
      "Processed 145/400 samples...\r\n",
      "Processed 150/400 samples...\r\n",
      "Processed 155/400 samples...\r\n",
      "Processed 160/400 samples...\r\n",
      "Processed 165/400 samples...\r\n",
      "Processed 170/400 samples...\r\n",
      "Processed 175/400 samples...\r\n",
      "Processed 180/400 samples...\r\n",
      "Processed 185/400 samples...\r\n",
      "Processed 190/400 samples...\r\n",
      "Processed 195/400 samples...\r\n",
      "Processed 200/400 samples...\r\n",
      "Processed 205/400 samples...\r\n",
      "Processed 210/400 samples...\r\n",
      "Processed 215/400 samples...\r\n",
      "Processed 220/400 samples...\r\n",
      "Processed 225/400 samples...\r\n",
      "Processed 230/400 samples...\r\n",
      "Processed 235/400 samples...\r\n",
      "Processed 240/400 samples...\r\n",
      "Processed 245/400 samples...\r\n",
      "Processed 250/400 samples...\r\n",
      "Processed 255/400 samples...\r\n",
      "Processed 260/400 samples...\r\n",
      "Processed 265/400 samples...\r\n",
      "Processed 270/400 samples...\r\n",
      "Processed 275/400 samples...\r\n",
      "Processed 280/400 samples...\r\n",
      "Processed 285/400 samples...\r\n",
      "Processed 290/400 samples...\r\n",
      "Processed 295/400 samples...\r\n",
      "Processed 300/400 samples...\r\n",
      "Processed 305/400 samples...\r\n",
      "Processed 310/400 samples...\r\n",
      "Processed 315/400 samples...\r\n",
      "Processed 320/400 samples...\r\n",
      "Processed 325/400 samples...\r\n",
      "Processed 330/400 samples...\r\n",
      "Processed 335/400 samples...\r\n",
      "Processed 340/400 samples...\r\n",
      "Processed 345/400 samples...\r\n",
      "Processed 350/400 samples...\r\n",
      "Processed 355/400 samples...\r\n",
      "Processed 360/400 samples...\r\n",
      "Processed 365/400 samples...\r\n",
      "Processed 370/400 samples...\r\n",
      "Processed 375/400 samples...\r\n",
      "Processed 380/400 samples...\r\n",
      "Processed 385/400 samples...\r\n",
      "Processed 390/400 samples...\r\n",
      "Processed 395/400 samples...\r\n",
      "Processed 400/400 samples...\r\n",
      "\r\n",
      "===== FINAL EVALUATION RESULTS =====\r\n",
      "Accuracy: 0.9225\r\n",
      "F1 Score: 0.0000\r\n",
      "Precision: 0.0000\r\n",
      "Recall: 0.0000\r\n",
      "Confusion Matrix:\r\n",
      "[[369   0]\r\n",
      " [ 31   0]]\r\n",
      "\r\n",
      "True Positives: 0\r\n",
      "True Negatives: 369\r\n",
      "False Positives: 0\r\n",
      "False Negatives: 31\r\n",
      "Specificity (True Negative Rate): 1.0000\r\n",
      "\r\n",
      "===== EXAMPLE PREDICTIONS =====\r\n",
      "\r\n",
      "Sample 1:\r\n",
      "True label: 0\r\n",
      "Predicted: 0\r\n",
      "Generated text: user\r\n",
      "Analyze the following survey response and determine if it is a quality response.\r\n",
      "\r\n",
      "Q1. What is y...\r\n",
      "\r\n",
      "Sample 2:\r\n",
      "True label: 0\r\n",
      "Predicted: 0\r\n",
      "Generated text: user\r\n",
      "Analyze the following survey response and determine if it is a quality response.\r\n",
      "\r\n",
      "Q1. What is y...\r\n",
      "\r\n",
      "Sample 3:\r\n",
      "True label: 0\r\n",
      "Predicted: 0\r\n",
      "Generated text: user\r\n",
      "Analyze the following survey response and determine if it is a quality response.\r\n",
      "\r\n",
      "Q1. What is y...\r\n",
      "\r\n",
      "Sample 4:\r\n",
      "True label: 0\r\n",
      "Predicted: 0\r\n",
      "Generated text: user\r\n",
      "Analyze the following survey response and determine if it is a quality response.\r\n",
      "\r\n",
      "Q1. What is y...\r\n",
      "\r\n",
      "Sample 5:\r\n",
      "True label: 0\r\n",
      "Predicted: 0\r\n",
      "Generated text: user\r\n",
      "Analyze the following survey response and determine if it is a quality response.\r\n",
      "\r\n",
      "Q1. What is y...\r\n",
      "\r\n",
      "Detailed validation results saved to ./gemma-9b-survey-finetuned/validation_results.csv\r\n",
      "\r\n",
      "Fine-tuning and evaluation complete!\r\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=2 --master_port=29500 my_script.py  # Replace with your actual Python script name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee98ef6",
   "metadata": {
    "papermill": {
     "duration": 0.065377,
     "end_time": "2025-03-10T10:40:31.384634",
     "exception": false,
     "start_time": "2025-03-10T10:40:31.319257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6830885,
     "sourceId": 10977315,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3818.386158,
   "end_time": "2025-03-10T10:40:31.873396",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-10T09:36:53.487238",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
