{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-03-09T16:46:50.177122Z","iopub.status.busy":"2025-03-09T16:46:50.176788Z","iopub.status.idle":"2025-03-09T16:46:50.180833Z","shell.execute_reply":"2025-03-09T16:46:50.179979Z","shell.execute_reply.started":"2025-03-09T16:46:50.177094Z"},"trusted":true},"outputs":[],"source":["import os\n","os.environ[\"HF_TOKEN\"] = \"token\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install -U bitsandbytes"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T16:46:18.994943Z","iopub.status.busy":"2025-03-09T16:46:18.994503Z","iopub.status.idle":"2025-03-09T16:46:23.374225Z","shell.execute_reply":"2025-03-09T16:46:23.373530Z","shell.execute_reply.started":"2025-03-09T16:46:18.994903Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import torch\n","import transformers\n","import pandas as pd\n","\n","\n","ce_loss_fn = torch.nn.CrossEntropyLoss(reduction=\"none\")\n","softmax_fn = torch.nn.Softmax(dim=-1)\n","\n","\n","def perplexity(encoding: transformers.BatchEncoding,\n","               logits: torch.Tensor,\n","               median: bool = False,\n","               temperature: float = 1.0):\n","    shifted_logits = logits[..., :-1, :].contiguous() / temperature\n","    shifted_labels = encoding.input_ids[..., 1:].contiguous()\n","    shifted_attention_mask = encoding.attention_mask[..., 1:].contiguous()\n","\n","    if median:\n","        ce_nan = (ce_loss_fn(shifted_logits.transpose(1, 2), shifted_labels).\n","                  masked_fill(~shifted_attention_mask.bool(), float(\"nan\")))\n","        ppl = np.nanmedian(ce_nan.cpu().float().numpy(), 1)\n","\n","    else:\n","        ppl = (ce_loss_fn(shifted_logits.transpose(1, 2), shifted_labels) *\n","               shifted_attention_mask).sum(1) / shifted_attention_mask.sum(1)\n","        ppl = ppl.to(\"cpu\").float().numpy()\n","\n","    return ppl\n","\n","\n","def entropy(p_logits: torch.Tensor,\n","            q_logits: torch.Tensor,\n","            encoding: transformers.BatchEncoding,\n","            pad_token_id: int,\n","            median: bool = False,\n","            sample_p: bool = False,\n","            temperature: float = 1.0):\n","    vocab_size = p_logits.shape[-1]\n","    total_tokens_available = q_logits.shape[-2]\n","    p_scores, q_scores = p_logits / temperature, q_logits / temperature\n","\n","    p_proba = softmax_fn(p_scores).view(-1, vocab_size)\n","\n","    if sample_p:\n","        p_proba = torch.multinomial(p_proba.view(-1, vocab_size), replacement=True, num_samples=1).view(-1)\n","\n","    q_scores = q_scores.view(-1, vocab_size)\n","\n","    ce = ce_loss_fn(input=q_scores, target=p_proba).view(-1, total_tokens_available)\n","    padding_mask = (encoding.input_ids != pad_token_id).type(torch.uint8)\n","\n","    if median:\n","        ce_nan = ce.masked_fill(~padding_mask.bool(), float(\"nan\"))\n","        agg_ce = np.nanmedian(ce_nan.cpu().float().numpy(), 1)\n","    else:\n","        agg_ce = (((ce * padding_mask).sum(1) / padding_mask.sum(1)).to(\"cpu\").float().numpy())\n","\n","    return agg_ce"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T16:46:23.375623Z","iopub.status.busy":"2025-03-09T16:46:23.375228Z","iopub.status.idle":"2025-03-09T16:46:27.209468Z","shell.execute_reply":"2025-03-09T16:46:27.208725Z","shell.execute_reply.started":"2025-03-09T16:46:23.375593Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","\n","def assert_tokenizer_consistency(model_id_1, model_id_2):\n","    identical_tokenizers = (\n","            AutoTokenizer.from_pretrained(model_id_1).vocab\n","            == AutoTokenizer.from_pretrained(model_id_2).vocab\n","    )\n","    if not identical_tokenizers:\n","        raise ValueError(f\"Tokenizers are not identical for {model_id_1} and {model_id_2}.\")\n","        "]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T16:46:37.143700Z","iopub.status.busy":"2025-03-09T16:46:37.143157Z","iopub.status.idle":"2025-03-09T16:46:37.252795Z","shell.execute_reply":"2025-03-09T16:46:37.251911Z","shell.execute_reply.started":"2025-03-09T16:46:37.143651Z"},"trusted":true},"outputs":[],"source":["from typing import Union\n","\n","import os\n","import numpy as np\n","import torch\n","import transformers\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","torch.set_grad_enabled(False)\n","\n","huggingface_config = {\n","    # Only required for private models from Huggingface (e.g. LLaMA models)\n","    \"TOKEN\": os.environ.get(\"HF_TOKEN\", None)\n","}\n","\n","# selected using Falcon-7B and Falcon-7B-Instruct at bfloat16\n","BINOCULARS_ACCURACY_THRESHOLD = 0.9015310749276843  # optimized for f1-score\n","BINOCULARS_FPR_THRESHOLD = 0.8536432310785527  # optimized for low-fpr [chosen at 0.01%]\n","\n","DEVICE_1 = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","DEVICE_2 = \"cuda:1\" if torch.cuda.device_count() > 1 else DEVICE_1\n","\n","\n","class Binoculars(object):\n","    def __init__(self,\n","                 observer_name_or_path: str = \"google/gemma-2-2b\",\n","                 performer_name_or_path: str = \"google/gemma-2-2b-it\",\n","                 use_bfloat16: bool = True,\n","                 max_token_observed: int = 512,\n","                 mode: str = \"low-fpr\",\n","                 ) -> None:\n","        assert_tokenizer_consistency(observer_name_or_path, performer_name_or_path)\n","\n","        self.change_mode(mode)\n","        self.observer_model = AutoModelForCausalLM.from_pretrained(\n","                observer_name_or_path,\n","                device_map={\"\": DEVICE_1},\n","                trust_remote_code=True,\n","                load_in_4bit=True,  # Enable 4-bit quantization\n","                bnb_4bit_compute_dtype=torch.float16,  # or torch.bfloat16 based on your preference\n","                token=huggingface_config[\"TOKEN\"]\n","            )\n","\n","        self.performer_model = AutoModelForCausalLM.from_pretrained(\n","                performer_name_or_path,\n","                device_map={\"\": DEVICE_2},\n","                trust_remote_code=True,\n","                load_in_4bit=True,  # Enable 4-bit quantization\n","                bnb_4bit_compute_dtype=torch.float16,\n","                token=huggingface_config[\"TOKEN\"]\n","            )\n","\n","\n","        self.observer_model.eval()\n","        self.performer_model.eval()\n","\n","        self.tokenizer = AutoTokenizer.from_pretrained(observer_name_or_path)\n","        if not self.tokenizer.pad_token:\n","            self.tokenizer.pad_token = self.tokenizer.eos_token\n","        self.max_token_observed = max_token_observed\n","\n","    def change_mode(self, mode: str, custom_threshold: float = None) -> None:\n","        if mode == \"low-fpr\":\n","            self.threshold = BINOCULARS_FPR_THRESHOLD\n","        elif mode == \"accuracy\":\n","            self.threshold = BINOCULARS_ACCURACY_THRESHOLD\n","        elif custom_threshold is not None:\n","            self.threshold = custom_threshold\n","        else:\n","            raise ValueError(f\"Invalid mode: {mode}\")\n","\n","\n","    def _tokenize(self, batch: list[str]) -> transformers.BatchEncoding:\n","        batch_size = len(batch)\n","        encodings = self.tokenizer(\n","            batch,\n","            return_tensors=\"pt\",\n","            padding=\"longest\" if batch_size > 1 else False,\n","            truncation=True,\n","            max_length=self.max_token_observed,\n","            return_token_type_ids=False).to(self.observer_model.device)\n","        return encodings\n","\n","    @torch.inference_mode()\n","    def _get_logits(self, encodings: transformers.BatchEncoding) -> torch.Tensor:\n","        observer_logits = self.observer_model(**encodings.to(DEVICE_1)).logits\n","        performer_logits = self.performer_model(**encodings.to(DEVICE_2)).logits\n","        if DEVICE_1 != \"cpu\":\n","            torch.cuda.synchronize()\n","        return observer_logits, performer_logits\n","\n","    def compute_score(self, input_text: Union[list[str], str]) -> Union[float, list[float]]:\n","        batch = [input_text] if isinstance(input_text, str) else input_text\n","        encodings = self._tokenize(batch)\n","        observer_logits, performer_logits = self._get_logits(encodings)\n","        ppl = perplexity(encodings, performer_logits)\n","        x_ppl = entropy(observer_logits.to(DEVICE_1), performer_logits.to(DEVICE_1),\n","                        encodings.to(DEVICE_1), self.tokenizer.pad_token_id)\n","        binoculars_scores = ppl / x_ppl\n","        binoculars_scores = binoculars_scores.tolist()\n","        return binoculars_scores[0] if isinstance(input_text, str) else binoculars_scores\n","\n","    def predict(self, input_text: Union[list[str], str]) -> Union[list[str], str]:\n","        binoculars_scores = np.array(self.compute_score(input_text))\n","        pred = np.where(binoculars_scores < self.threshold,\n","                        \"Most likely AI-generated\",\n","                        \"Most likely human-generated\"\n","                        ).tolist()\n","        return pred\n","    def predict_with_confidence(self, input_text: Union[list[str], str]) -> Union[list[tuple], tuple]:\n","        binoculars_scores = self.compute_score(input_text)\n","        \n","        # Handle single string vs list of strings appropriately\n","        if isinstance(input_text, str):\n","            # For a single string, binoculars_scores is a scalar\n","            score = binoculars_scores\n","            distance_from_threshold = abs(score - self.threshold)\n","            scaling_factor = 5.0\n","            confidence = (1 - np.exp(-scaling_factor * distance_from_threshold)) * 100\n","            prediction = \"Most likely AI-generated\" if score < self.threshold else \"Most likely human-generated\"\n","            return (prediction, confidence)\n","        else:\n","            # For a list, convert to numpy array for vectorized operations\n","            scores = np.array(binoculars_scores)\n","            distance_from_threshold = abs(scores - self.threshold)\n","            scaling_factor = 5.0\n","            confidence = (1 - np.exp(-scaling_factor * distance_from_threshold)) * 100\n","            predictions = np.where(scores < self.threshold,\n","                                  \"Most likely AI-generated\",\n","                                  \"Most likely human-generated\"\n","                                  ).tolist()\n","            return list(zip(predictions, confidence.tolist()))"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T16:46:53.977420Z","iopub.status.busy":"2025-03-09T16:46:53.977126Z","iopub.status.idle":"2025-03-09T16:48:45.790446Z","shell.execute_reply":"2025-03-09T16:48:45.789722Z","shell.execute_reply.started":"2025-03-09T16:46:53.977396Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c1520d0a5fc84efebd7e4d7399aec779","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d1cebafa54e4656892f74343073aee7","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a6578976921944308b3a5b985544e45a","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"403f9d194cb1426289c0c208ef1b6002","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0aad6ccc57a54f4fbe53817b5488c9d4","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f8de0655daa347cca373ac7b91528911","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a8eece4d69464c03937c3b9624fec62d","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c83722b7d62640fdb3ea36ef6408f4e5","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8af403ac7ec64a58aaadd71c4c4ea5b2","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/818 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1dbd3c8dd2064e37a899cb9ab01ff6fc","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f9ae4d335a2640cfa7135db6b427de8d","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"43a43b93326c40288071aae6c1fa294b","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b7113e7c79b4b46a19729bca727d8af","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b2bff7c295b8429c8d0fa986221c6b18","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00003.safetensors:   0%|          | 0.00/481M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ed0962eda48454f95e2e87e5bb0dcc0","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"31a9f81b40b24f90bffeea7be43d9678","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0083d92949484fbb87c839b29dfc25d8","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2fc00ac57b394a3fafff03dd4ddca5ac","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"68241a9b8d7446d4b3b83f98d528882a","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c8d3a1b9b724bf0905d761a057c3acf","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"547bf49b6b574c51934c20ee5fb55df4","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7983cef253124700b138e9e9ec0e969a","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fbabc420b69c42fd8ee1130952b3660c","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["bino = Binoculars()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T16:48:57.523952Z","iopub.status.busy":"2025-03-09T16:48:57.523330Z","iopub.status.idle":"2025-03-09T16:48:57.548605Z","shell.execute_reply":"2025-03-09T16:48:57.547709Z","shell.execute_reply.started":"2025-03-09T16:48:57.523920Z"},"trusted":true},"outputs":[],"source":["data_path = \"/kaggle/input/traindataset/trainData.csv\"\n","df = pd.read_csv(data_path)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T16:49:58.568434Z","iopub.status.busy":"2025-03-09T16:49:58.568136Z","iopub.status.idle":"2025-03-09T16:49:58.575164Z","shell.execute_reply":"2025-03-09T16:49:58.574310Z","shell.execute_reply.started":"2025-03-09T16:49:58.568412Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from collections import Counter\n","\n","def row_to_values_list(row, columns_to_include):\n","    \"\"\"\n","    Extract values from specified columns in a row, replace 'nan' with 'No response',\n","    and return them as a list.\n","    \"\"\"\n","    values = []\n","    for col in columns_to_include:\n","        value = row[col]\n","        if pd.isna(value):\n","            value = 'No response'\n","        else:\n","            value = str(value)\n","        values.append(value)\n","    return values\n","\n","def aggregation(predictions):\n","    \"\"\"\n","    Perform maximum voting aggregation on predictions.\n","    \n","    Args:\n","        predictions: List of tuples in the form (\"label\", confidence_score)\n","    \n","    Returns:\n","        A tuple with the most frequent label and its corresponding highest confidence score.\n","    \"\"\"\n","    label_counts = Counter()\n","    max_confidence = {\"Most likely human-generated\": 0, \"Most likely AI-generated\": 0}\n","    \n","    for label, confidence in predictions:\n","        print(label,confidence)\n","        label_counts[label] += 1\n","        max_confidence[label] = max(max_confidence[label], confidence)\n","    \n","    # Select the most common label\n","    most_common_label = label_counts.most_common(1)[0][0]\n","    \n","    return most_common_label, max_confidence[most_common_label]\n","\n","def aiDetector(row_number, df, columns_of_interest, bino):\n","    \"\"\"\n","    Detect AI-generated responses for a specific row using an aggregation function.\n","    \n","    Args:\n","        row_number: Index of the row to analyze\n","        df: DataFrame containing the survey data\n","        columns_of_interest: List of columns to extract values from\n","        bino: Object with methods `predict_with_confidence` and `compute_score`\n","    \n","    Returns:\n","        Aggregated result from the aggregation function.\n","    \"\"\"\n","    row = df.iloc[row_number]\n","    responses = row_to_values_list(row, columns_of_interest)\n","    \n","    predictions = []\n","    for response in responses:\n","        label, confidence = bino.predict_with_confidence(response)\n","        predictions.append((label, confidence))\n","    \n","    return aggregation(predictions)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T16:50:00.443492Z","iopub.status.busy":"2025-03-09T16:50:00.443154Z","iopub.status.idle":"2025-03-09T16:50:01.445999Z","shell.execute_reply":"2025-03-09T16:50:01.445105Z","shell.execute_reply.started":"2025-03-09T16:50:00.443463Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Most likely AI-generated 17.70006455776596\n","Most likely human-generated 97.99496192050546\n","Most likely human-generated 99.68653231955265\n","Most likely human-generated 99.43385593792105\n","Most likely human-generated 99.99998764613231\n","Most likely human-generated 99.9999702623447\n"]},{"data":{"text/plain":["('Most likely human-generated', 99.99998764613231)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Example usage\n","columns_of_interest = ['Likes About Concept', 'Dislikes About Concept', 'Concept Replacement Product', 'Replacement Product 1', 'Replacement Product 2', 'Replacement Product 3']\n","row_number = 4\n","result = aiDetector(row_number, df, columns_of_interest, bino)\n","result"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T16:52:37.364564Z","iopub.status.busy":"2025-03-09T16:52:37.364276Z","iopub.status.idle":"2025-03-09T16:52:37.558879Z","shell.execute_reply":"2025-03-09T16:52:37.558175Z","shell.execute_reply.started":"2025-03-09T16:52:37.364542Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":6801394,"sourceId":10937104,"sourceType":"datasetVersion"}],"dockerImageVersionId":30919,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
